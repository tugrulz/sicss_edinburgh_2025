---
title: "1-analysis"
author: "Artemis_Deligianni"
date: "2025-05-21"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

# Introduction to Data Analysis using R and Tidyverse

In this tutorial, we will learn how to do a simple data analysis using tidyverse

**tidyverse** is an R library that makes it easy to work with structured data—like what you’d see in Excel spreadsheets or SQL tables.

In R universe, tables are called a dataframe. It consists of rows and columns. 
You can create a dataframe from lists, dicts, or other data types. However usually you creating using a dataset you collected. 

You can load CSVs (comma separated values), JSONs, Excel Files (xlsx). 
You can compress csvs and jsons using bzip (so they will be .csv.bz2 or .json.bz2) and still load them as usual. R handles uncompressing without modifying the files.

We will use a Twitter (now X) dataset that consists of 500k tweets related to ChatGPT between January and March 2023. [Link](https://www.kaggle.com/datasets/khalidryder777/500k-chatgpt-tweets-jan-mar-2023)

Let's load our sample data using read_csv() function.


As the data consists of 500,000 entries and is 117 mb when uncompressed, it is better to use a sample for learning and exploration purposes. You can first load the data and then take a random sample. However, in a scenario where the data is massive (more than a gigabyte), it is more practical to load the first n rows using `nrows` argument of `read_csv` function.
We load the first 10000 rows. 


```{r}
#here we load the packages we need for our analyses
library(tidyverse)
```

```{r}
df <- read.csv(file="./data/twitter_data_chatgpt.csv.bz2",nrows=10000)
#the nice thing with R is you can interact with the dataframe table it prints
```
You will notice below I have set include=FALSE into the R chunk, this is to avoid printing the whole dataframe into the html document when you knit it. R will hide the chunk and not include it in the document. 

Here are other cool options available to you as listed in [LINK]("https://rmarkdown.rstudio.com/lesson-3.html"):

include = FALSE prevents code and results from appearing in the finished file. R Markdown still runs the code in the chunk, and the results can be used by other chunks.
echo = FALSE prevents code, but not the results from appearing in the finished file. This is a useful way to embed figures.
message = FALSE prevents messages that are generated by code from appearing in the finished file.
warning = FALSE prevents warnings that are generated by code from appearing in the finished

```{r include=FALSE}
df
```

If you want to include a small table of your dataframe you can do: 

```{r}
head(df)
```


Every dataframe has an index, which uniquely identifies each row and plays an important role in data alignment and operations like selection, joining, and reshaping (that we will see later)

By default, if you don't specify an index, R assigns one automatically: a sequence of integers from 1 to n, where n is the number of rows. (This is called a RangeIndex.) 

NOTE: In R indexing starts from 1 not 0 ! 

```{r}
range(row_number(df))
```


In R the index is not a column of the dataframe; you can open the df from the environment tab by clicking on it and check it out for yourselves!

```{r}
df$index
```


You can specify the index explicitly by `rownames`` function (if you have not done while reading the csv). 

An index should be unique, i.e., should map to a single row. Twitter assigns an unique id to each tweet, which is an excellent candidate for an index


```{r}
rownames(df) <- df$id
```

Because R treats the index as not a column you will notice the id column is retained. This is by design because the index operates similarly regardless of the assigned values, but the id column is necessary to perform activities like searching for a row by id etc. 

Note that R operations like `rownmaes` return the new (modified) dataframe after calling a function without the need to assign it back to the data frame variable. Other functions, like `summarise` will require you to save the output back into a variable. We will look at these later on. 

```{r  results='hide'}
#indexing in R
df[1] #retusn first column with title etc.

df[1,] #returns first row

df[,1] #returns first column entries

df[1,1] #print first row of first column

df[10000, 1] #print last row of first column
```
Notice here that even though we made the index value equal the id values, the indexing still works using values between 1 and 10000. 

If you want to return a certain row based on the id you can use: 

```{r}
df[df$id==1641213230730051584,]
```

You can access multiple rows using a list
```{r}
ids <- c(1641213003260633088, 1641212975012016128, 1641213230730051584)

df[df$id %in% ids,]
```

If you want to access rows by name

```{r results='hide'}
df$id #single row

```
```{r results='hide'}
df%>%select(id, date) #multiple rows 
#OR 
df[c("id", "date")]

```

You will notice in the second example we used %>% this is called a pipe in R and its main function is to funnel information from the variable/function to its left to the variable/function to the right. This might take some time to get used to especially if you have more experience in Python but with practise it will become more intuitive. It is worth getting familiar with this as it is a very powerful method for all sorts of things like filtering, summarising, mutating and more. 

You saw above we have use the function `head` to return the first 6 rows. We can also return the last 6 rows of our dataframe:

```{r}

tail(df)

```


Also note `length` function will give you the number of columns same as `ncol`, whereas `nrow` returns the length of the dataframe.
```{r}
length(df)
ncol(df)
nrow(df)
```

The get the actual shape or dimensions of the dataframe we use `dim`
```{r}
dim(df)
```

You can also get the names of all columns in your dataframe
```{r}
names(df)
```
You can save them as a list like so

```{r}
column_list<-list(names(df))
```

`summary` can provide some general information about the dataframe, such as the classes of variables and if applicable descriptive stats.
```{r}
summary(df)
```
Let's now look at another useful way of using the %>%. We can go into the dataframe, group our observations by the usernames and count the number of observations we have for each user

```{r}
df%>%group_by(username)%>%count()
```
R automatically presents the usernames in alphabetic order, starting with numerical values and going down the alphabet. 

We can also sort this by count and see who is the most prolific user in our dataset. We can do this using again the %>% pipe function together with the summarise() function and telling R to arrange the counts in descending order. 


```{r}
users<-df%>%group_by(username)%>%summarise(N=n())%>%arrange(desc(N))
head(users)

```
This says that __yuhanito__ and __torksmith__ are the most occurring value in username. In other words, they tweeted the most (in the first 10000 rows of the dataset)

Here we saved the output into a new variable called users so we can save it in our environment (to the right) otherwise R will only ever print the outcome once and we won't be able to access it again. You can open the users variable and have a look at all the users and their respective post counts if you'd like

#Analysis

The data we loaded consists of only the first 10000 rows and the data is sorted according to the date so we only loaded the tweets posted in March. It does not provide reliable findings for a temporal analysis. 
Thus, we will load the full data this time, but sample later for fast analysis.

We load the first 10000 rows to a variable named `small_df` for comparison purposes that you will see soon


```{r}
df<-read.csv("./data/twitter_data_chatgpt.csv.bz2")
small_df<-read.csv(file = "./data/twitter_data_chatgpt.csv.bz2",nrows=10000 ) #for comparison purposes
```

## Data Preprocessing
When dealing with data, you will invest a great deal of time and effort to preprocessing: cleaning the data, creating features and making the data ready to whatever analysis you will do. It is the fundamental part of data engineering.
AI made this part easy, but it is important to learn these skills, so you will know what you are doing.


### Cleaning
The data provided in Kaggle is in fact dirty. Normally, the string columns should be wrapped in double quotes so the newlines (\n) in the text will be recognized as part of the value instead of indicating a new row in a dataframe. The author of the dataset broke this principle in some rows. This resulted in columns mixing together, e.g., the username became date, date became id and so on. We will now clean this mess.

```{r  results='hide'}
df
```

Compare with the data types in the dataframe of first 10000 rows (which we named `small_df`). You can use the `lapply` function to apply the `class` function to all the variables in the dataframe.
```{r}
lapply(df, class) 

lapply(small_df, class) 
```

Because of the unexpected linebreaks in the column `content`, some rows got splitted in two. This made the superseding columns had null values. 
We can identify these rows using is.na(). 
is.na() creates a dataframe of booleans.

```{r  results='hide'}
is.na(df)
```
```{r}
#check spefically for NAs
df %>% filter(if_any(everything(), is.na))
```

WE have 62 rows containing NAs. If we open the dataframe and sort like_count in ascending order, we see that for example line 34984 is broken into three and carries over to 34985 and 34986
The line 56153 is broken into three and carries over to 56154 and 56155
The line 114179 is broken into two and carries over to 114180
... and so on.

What to do in such a situation? In fact, Twitter and nearly all other major social media platforms such as Reddit, YouTube, TikTok provide data in JSON format which is robust to this problem. If you have the original (raw) data, the best solution is to reread the data and create a clean csv. Since the author of the dataset did not provide such data, we have to move on.

The other solution is to read the csv line by line and fix the bad lines, create a clean csv and then load it.

The easiest solution is to drop the erroneous lines and fix the columns' class This is not a good solution if there are many affected rows. However, in our case, it is only 62 tweets out of 500,000. So we will go with this solution. It also provides a good exercise for other cleaning steps


#### Dropping Rows with Null Values
Drop the rows with null values using dropna()

```{r}
number_of_rows_earlier = nrow(df)
df<-na.omit(df)
rows_dropped = number_of_rows_earlier - nrow(df)
cat('Number of rows dropped:', rows_dropped)
#alterntively you can do print(paste0('Number of rows dropped:', rows_dropped))
```
Now we fix the columns' data classes. R has correctly recognised that like_count and retweet_count are integer types. However id is also an integer and sometimes depending on the analyses we plan to do we might want it to be a factor. 

R has a series of functions that follow the naming convention as.class() to do this; here we use as.factor()

```{r}
df$id<-as.factor(df$id)

```

For now we can let it be numeric again

```{r}
df$id<-as.numeric(df$id)

```


Because R automatically asigns an index and because of how indexing works in R we will not be trying to make the id column our index. We will keep the id column as is; in many situations the id helps us group our data in a meaningful way, for example if we have multiple observations per individual participant in a study or if the id was representing individual users (the way username does here).

#### Renaming columns
An important preprocessing step to give your columns clear names that will be compatible with additional data. 
Twitter API names tweet creation date as "created_at" and tweet text as "text". The Kaggle author did a poor job in naming those fields. "date" may be confused with the type date and  and "content" is not used elsewhere. So we will fix those

Use rename() to rename columns. Use a dictionary where the keys will refer to the current names and values refer to the new names. 


```{r}
df<-df%>%rename(created_at=date,
                text=content)

head(df)
```


#### Binfinh Dataframes
In a scenario where we need to join multiple dataframes into one, we can do so by using bind_rows().
We do not have such a scenario in hand right now but we can append the small dataframe to our dataframe to create duplicates for our next exercise.

We will assign the combined dataframe to a dataframe named combined_df. You will shortly see why.

```{r include=FALSE}
combined_df<-bind_rows(df, small_df) 

```

You will notice that rbind() throws an error. That is because our dataframe columns and classes do not much.

To bind datframes in R is to ensure their column names and data types match so we will first rename the columns in small_df as well.
```{r}
small_df<-small_df%>%rename(created_at=date,
                text=content)

cat('Length of the dataframe:', nrow(df),
    '\nLength of the small dataframe:', nrow(small_df),
    '\nLength of the combined dataframe is supposed to be:', nrow(df)+nrow(small_df))

combined_df<-bind_rows(df, small_df) 

cat('\nNew length of the dataframe:', nrow(combined_df))
```

Now look at the head and the tail of the combined dataframe. The head will show the rows from the first dataframe, and the tail will show the rows from the second, smaller one. 

What do you see?

```{r}
head(combined_df)
tail(combined_df)
```

Things look ok. R typically will complain when your dataframes don't match in structure and naming which prevents issues because it throws an error and forces you to go back and check your code. Though if you do more complicated dataframe merging things can get a bit trickier later on.


### Identifying and Dropping Duplicates
Duplicates may skew your results or may yield in meaningful results. 
You should consider what type of duplicates may be problematic for your analysis and define & drop duplicates accordingly. 

We will consider multiple cases of duplicates and learn how to deal with them.


#### 1) Duplicate IDs
In R, an index is unique for clarity, performance and correctness.

On Twitter, ids are serve as index: they are unique and are used as the address of the tweet, e.g., "1895669466786402519" in [https://x.com/TheMisterFrog/status/1895669466786402519](https://x.com/TheMisterFrog/status/1895669466786402519) is the tweet id.

If your Twitter dataset has rows with the same ids (which we just set as index), either they are the same exact tweets or tweet ids are stored or read incorrectly.

We do not have this problem in our main dataset. However combined_df has it since we appended a subset of the data, creating many duplicates.



```{r}
dim(df[duplicated(df$id),]) # rows should be 0 

dim(combined_df[duplicated(combined_df$id),] ) # rows should be 3002

#you can remove dim if you'd like to print the actual rows. 
```

To remove duplicated rows by id we will use the distinct() function that allows us to select which column we are dropping duplicates from. We use all equal which will tell us if the two dfs are now equal and if not why. 

```{r}
combined_df<-combined_df%>%distinct(id, .keep_all = TRUE)

all.equal(df, combined_df)
```
Notice here that the dfs don't match! Why do you think that is? Pause to think before continuing on. (Hint: it is to do with the file we're using)



The csv file we loaded is broken! R didn't throw an error when loading it because we used read.csv() as opposed to read_csv(). Below we load the file with read_csv() and we automatically skip empty rows caused by the broken file.

```{r}

df<-read_csv("./data/twitter_data_chatgpt.csv.bz2", skip_empty_rows = TRUE)
df[duplicated(df$id),] #when loading it this way there's actually 5 rows which are duplicated so 
df<-df%>%distinct(id, .keep_all = TRUE)
df<-na.omit(df) # no NAs
small_df<-df %>% slice(1:10000)

class(df$id)
class(small_df$id)

#bind, make id as factor, group and then remove duplicates with slice()
combined_df <- bind_rows(df, small_df)

combined_df<-combined_df%>%
  mutate(id = as.numeric(id)) %>%
  group_by(id) %>%
  slice(1) %>%
  ungroup()

```
Now, we can see that your initial dataframe matches in length to the cleaned combined_df. We have successfully removed all duplicated ids. 


#### Duplicate Rows
In some cases, the index (e.g., id) may be unique but the values are the same. Interestingly, this is the case in our dataset:

```{r}

df<-df%>%rename(created_at=date,
                text=content)
df$id<-as.factor(df$id)

df[duplicated(df$text),]

```
```{r}

#notice that in R if you use duplicate on the whole df it will onlky return TRUE if all the columns for 2 rows match exactly which in our data they do not.
df$is_duplicate <- duplicated(df)
df%>%filter(is_duplicate==TRUE)%>%head()
```


Seems like a clumsily programmed Twitter bot got a fatal error and tweeted "@gpt_chatgpt Response exceeds tweet character limit" 11 times (plus 2 tweets from random people). 
This is not a big deal for a dataset of 500,000 tweets. But let's drop them anyway.
This one is easy, just call `distinct()`.

```{r}
df<-df%>%distinct(text, .keep_all = TRUE)

```


distinct() only works with 1 column, if we want to remove duplicates based on multiple rows we need to use duplicated() and logic (in this case the oppisite ! sign). 
```{r}
 df<- df[!duplicated(df[, c("username", "text")]), ]

```


### Creating and Dropping Columns
You can create new columns using a simple assignment statement.
The following will assign the same value to each for the new column. So, a constant column

```{r}
df$year=2023
```

You can also use the preexisting columns to create new columns by simple operations


```{r}
df$engagement_count<-df$like_count+ df$retweet_count
df$like_retweet_difference<-df$like_count-df$retweet_count
df$like_retweet_ratio<-df$like_count/df$retweet_count
head(df)
```
The year column was a bit useless, let's drop it. 
You can do this by assinging the column value to NULL

```{r}
df$year<- NULL
head(df)
```

.. or drop multiple columns

```{r}
df[c('engagement_count', 'like_retweet_difference')]<-NULL
head(df)
```
### Handling Missing Values or NaNs (Not a Number)s
In the beginning of the notebook, we handled the bad rows by dropping nas. In some cases, the data is read correctly, the rows are ok. Some values are either missing or there are problems in their computation.

In our case, when we compute like_retweet_ratio, we received a lot of NaNs by dividing zero by zero as many tweets have zero likes and zero retweets. 

```{r}
cat('Number of NaNs in like_retweet_ratio:', sum(is.na(df$like_retweet_ratio)))
```

One obvious solution is to smooth the values before dividing, e.g., add 1 to like and retweet count and then delete. 

However, if you instead opt for a solution where you treat tweets with zero likes and retweets, you can instead fill the NaN values with -1 (or leave them as it is)

You can replace NAs with `mutate_if` and `replace`

```{r}
df<-df%>% mutate_if(is.numeric,~replace(., is.na(.), -1))
head(df)
```

There are other strategies for filling missing values, e.g., filling with the column's mean or median.You can explore these in your own time. 

### Creating Complex Columns
You often create new columns for complex analysis or to use as feature for machine learning. Use apply() functions for that.

You can define new functions and use them in the apply() function to apply it to a column. The following function identifies and returns a list of hashtags in the tweet.


```{r}
extract_hashtags <- function(text) {
  hashtags <- c()  # initialize an empty vector to store hashtags
  splitted <- unlist(strsplit(text, " "))  # split text into words using space
  
  for (word in splitted) { # loop through each word
    if (startsWith(word, "#") && nchar(word) > 1) { # check if the word starts with # and longer than 1
      hashtags <- c(hashtags, word)  # add the word to the list of hashtags  
    }
  }
  
  return(hashtags)# return the list of hashtags
}

```


```{r}
df$hashtags<- apply(df, 1, extract_hashtags)  # 1 = rows, 2 = columns

head(df)
```


#### Custom function with multiple columns
What if you need to use multiple columns as input to the function you provide to apply()? We mix the two approaches we just learn.

First, let's say we want to identify "self-replies", the tweets which contain the username of their author. This is often the case when users author a Twitter flood / thread.

We define a custom function for that:

```{r}
identify_self_replies <- function(text, username) {
  reply_username <- paste0("@", username)  # replies start with @
  startsWith(text, reply_username)  # returns true if the text starts with the @username, indicates a self-reply
}

```

Because we now have 2 viarbles to apply this function to we will use `mapply`:
```{r}
df$is_self_reply<-mapply(identify_self_replies, df$text, df$username)
head(df)

```


Let's see how many self_replies we got:

```{r}
self_replies<-df%>%group_by(is_self_reply)%>%summarise(N=n())
self_replies
```
Seems like not many, but that's life :) 

This was the part one. Please proceed to part two.


