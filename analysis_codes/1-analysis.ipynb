{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction to Data Analysis using Pandas\n",
    "\n",
    "In this tutorial, we will learn how to do a simple data analysis using pandas.\n",
    "\n",
    "**pandas** is a Python library that makes it easy to work with structured data—like what you’d see in Excel spreadsheets or SQL tables.\n",
    "\n",
    "In Pandas universe, tables are called a dataframe. It consists of rows and columns. \n",
    "You can create a dataframe from lists, dicts, or other data types. However usually you creating using a data you collected. \n",
    "\n",
    "You can load CSVs (comma separated values), JSONs, Excel Files (xlsx). \n",
    "You can compress csvs and jsons using bzip (so they will be .csv.bz2 or .json.bz2) and still load them as usual. Pandas handles uncompressing without modifying the files.\n",
    "\n",
    "We will use a Twitter (now X) dataset that consists of 500k tweets related to ChatGPT between January and March 2023. [Link](https://www.kaggle.com/datasets/khalidryder777/500k-chatgpt-tweets-jan-mar-2023)\n",
    "\n",
    "Let's load our sample data using read_csv() function."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "beab93b332063573"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As the data consists of 500,000 entries and is 117 mb when uncompressed, it is better to use a sample for learning and exploration purposes. You can first load the data and then take a random sample. However, in a scenario where the data is massive (more than a gigabyte), it is more practical to load the first n rows using `nrows` argument of `read_csv` function.\n",
    "We load the first 10000 rows. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca299cbc86990cb2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1ef6f9651e60c62",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/twitter_data_chatgpt.csv.bz2', nrows = 10000) # use index_col to specify and index column\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61a2999d79061ac3",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Every dataframe has an index, which uniquely identifies each row and plays an important role in data alignment and operations like selection, joining, and reshaping (that we will see later)\n",
    "\n",
    "By default, if you don't specify an index, pandas assigns one automatically: a sequence of integers from 0 to n-1, where n is the number of rows. (This is called a RangeIndex.)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67063d6f67457fd1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.index"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97df55e6182cff81",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can specify the index explicitly by `set_index` function (if you have not done while reading the csv). \n",
    "\n",
    "An index should be unique, i.e., should map to a single row. Twitter assigns an unique id to each tweet, which is an excellent candidate for an index\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d966d22f9371c1a0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.set_index(\"id\", inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that some pandas operations like set_index return the new (modified) dataframe after calling a function.\n",
    "You need to assign it to a dataframe variable, e.g., `df = df.set_index(\"id\")`\n",
    "You can either do that, or use `inplace = True` to avoid it. Does not matter which one you choose, but it's better to be consistent. We will use the latter approach"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e02677b782c6cc4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can use the index to access certain rows. Use `.loc` for that."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec2954803ea2678f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.loc['1641213230730051584'] # the row with the specified index"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f183c5997e70923",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wait, why this did not work? Because id column was an integer :))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0b5d3e5d80584e4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.loc[1641213003260633088]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3be70562bb7c2a4c",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can access multiple rows using a list"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9297c5454c62746"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "ids = [1641213003260633088, 1641212975012016128, 1641213230730051584]\n",
    "df.loc[ids]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a600d203548456bd",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can also access certain rows using their numeric position. However this is not recommended because the numeric position may change if you sort or shuffle the data, which is not the case with an index.\n",
    "For that, use `.iloc` (stands for \"integer location\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e7c7d79d1cd7c17"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.iloc[0] #accesses the first row"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da136b839e0743a0",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use double brackets return a dataframe with single or multiple columns (useful when you want to access a column(s).\n",
    "Note: using a single bracket to access a single column returns a \"Series\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b1720b029d42e6b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df[['content']]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d923c62ceccafa8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df[['date', 'content']]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97b5f2d17af19c4f",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that once you set the id to be an index, it is no longer a column."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "863024256c091f61"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df[['id']] # will throw a KeyError"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "256ff714d3dab98",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Quick Exploration\n",
    "Here are some useful quick-exploration functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8cfd0fee0ec9ed"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Head shows the first rows, default 5, can be specified. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1c460d5d49a5d30"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.head() # first 5 rows"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f98f794c10d0f787",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "In fact, entering the variable of the dataframe defaults to calling its head function with n = 10"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d5378955f9377ce"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d0d8783e43d1c51",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tail does the same but return the last rows (defaults to 5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6b51473db3ddd74"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.tail() # last rows"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f1edfeff80fdadc",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Calling len() on a dataframe shows its number of rows."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a275e475d3155145"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "len(df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa4e3ed7fa60a273",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Shape is an attribute of a dataframe that holds information related to its size.\n",
    "It is a tuple. The first element shows the number of rows. The second element shows the number of columns. \n",
    "Our dataframe is 10000 x 5 so:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3349f2d714c290f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.shape # notice that we do not use parenthesis, as shape is an attribute not a function"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f8bbec222f8e472",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Columns attribute holds the column names"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eef6e0b406b82005"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.columns"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38580c2c62314cdb",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that this is a Pandas index. You can convert it to a list however"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e1c5167bcf739c6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.columns.to_list() # or list(df.columns)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cdd1e11d0a3e33c4",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "info() gives some info that may or may not be useful"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b20ebcdaa14ffa15"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.info()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c72340350a827c4d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "describe() provides statistics that are often useful"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "afbd09bc5268ea05"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.describe()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61833a10c39347eb",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "value_counts() provides an exhaustive **sorted** list of the values stored in column(s). It is quite meaningless to call it for the entire dataframe. However it is *very useful* in looking at the values in a single column. \n",
    "\n",
    "Let's see who are the most active users in this Twitter dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a51632ddac95ea3d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.username.value_counts()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "329fa71f68211d39",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This says that __yuhanito__ is the most occurring value in username. In other words, they tweeted the most (in the first 10000 rows of the dataset)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db40037dc57a846c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analysis\n",
    "The data we loaded consists of only the first 10000 rows and the data is sorted according to the date so we only loaded the tweets posted in March. It does not provide reliable findings for a temporal analysis. \n",
    "Thus, we will load the full data this time, but sample later for fast analysis.\n",
    "\n",
    "We load the first 10000 rows to a variable named `small_df` for comparison purposes that you will see soon"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8df74c815a49462f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/twitter_data_chatgpt.csv.bz2')\n",
    "small_df = pd.read_csv('data/twitter_data_chatgpt.csv.bz2', nrows = 10000) # for comparison purposes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aaf150a793c67701",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preprocessing\n",
    "When dealing with data, you will invest a great deal of time and effort to preprocessing: cleaning the data, creating features and making the data ready to whatever analysis you will do. It is the fundamental part of data engineering.\n",
    "AI made this part easy, but it is important to learn these skills, so you will know what you are doing."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81c6ccb13f673ee4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cleaning\n",
    "The data provided in Kaggle is in fact dirty. Normally, the string columns should be wrapped in double quotes so the newlines (\\n) in the text will be recognized as part of the value instead of indicating a new row in a dataframe. The author of the dataset broke this principle in some rows. This resulted in columns mixing together, e.g., the username became date, date became id and so on. We will now clean this mess."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f46211f190b04916"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "715f08880cb2d148",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Because the columns are messed up, their dtypes are not correct. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "193da29ab54c189f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.dtypes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68be517a097d0753",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Compare with the dtypes in the dataframe of first 10000 rows (which we named `small_df`)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80841d7b9b7741e9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "small_df.dtypes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e9d042f6bb26922",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Because of the unexpected linebreaks in the column `content`, some rows got splitted in two. This made the superseding columns had null values. \n",
    "We can identify these rows using isnull(). \n",
    "isnull() creates a dataframe of booleans.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f495dbe728e0a3b3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.isnull()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2043eb378367cbc8",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We want to get the rows where **any** value is null, i.e., each row of the boolean dataframe will aggregated and will return True if any of the columns has the value True. To do that, we use any(). Because we aggregate for each row on columns, we use axis = 1. # This is a bit confusing, rewrite"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad37f83dc13e38f1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.isnull().any(axis=1) # now we get a single bool for each row"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e1032a953116dc4",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using brackets on a list of bools will return the rows where the bool is True.\n",
    "In this case, the list of bools is a *mask*, which is used to *filter data*. We will go over this concept thoroughly later."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a85edf00d20dbcb9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df[df.isnull().any(axis=1)]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1711175d8b71795f",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we see that the line 34984 is broken into three and carries over to 34985 and 34986\n",
    "The line 56153 is broken into three and carries over to 56154 and 56155\n",
    "The line 114179 is broken into two and carries over to 114180\n",
    "... and so on."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d66800bd2f5b5b5b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "What to do in such a situation? In fact, Twitter and nearly all other major social media platforms such as Reddit, YouTube, TikTok provide data in JSON format which is robust to this problem. If you have the original (raw) data, the best solution is to reread the data and create a clean csv. Since the author of the dataset did not provide such data, we have to move on.\n",
    "\n",
    "The other solution is to read the csv line by line and fix the bad lines, create a clean csv and then load it.\n",
    "\n",
    "The easiest solution is to drop the erroneous lines and fix the columns' dtypes. This is not a good solution if there are many affected rows. However, in our case, it is only 20-30 tweets out of 500,000. So we will go with this solution. It also provides a good exercise for other cleaning steps"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfef196b01c86b90"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Dropping Rows with Null Values\n",
    "Drop the rows with null values using dropna()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f743f3a07238b05a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "number_of_rows_earlier = len(df)\n",
    "df.dropna(how = 'any', inplace = True)\n",
    "rows_dropped = number_of_rows_earlier - len(df)\n",
    "print(f'Number of rows dropped: {rows_dropped}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "686619e7cbdbf20d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we fix the columns' dtypes\n",
    "We use astype() to do this. Note that this we are reassigning the columns, we can't use inplace"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f9f11cb4fa1b140"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df['id'] = df['id'].astype(int)\n",
    "df['like_count'] = df['like_count'].astype(int)\n",
    "df['retweet_count'] = df['retweet_count'].astype(int)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c7cfef28e9db133",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we fixed the issue with the column id, we can again make it the index"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "904cb18d81c22bd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.set_index(\"id\", inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c21a8f37f2fa8605",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Renaming columns\n",
    "An important preprocessing step to give your columns clear names that will be compatible with additional data. \n",
    "Twitter API names tweet creation date as \"created_at\" and tweet text as \"text\". The Kaggle author did a poor job in naming those fields. \"date\" may be confused with the type date and  and \"content\" is not used elsewhere. So we will fix those\n",
    "\n",
    "Use rename() to rename columns. Use a dictionary where the keys will refer to the current names and values refer to the new names."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42d9ecd3a0d073c0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.rename(columns = {'date':'created_at', 'content': 'text'}, inplace = True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a77b8fb9979332d7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e37b117bec6aa57",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Concatenating Dataframes\n",
    "In a scenario where we need to combine multiple dataframes into one, we can do so by using concat().\n",
    "We do not have such a scenario in hand right now but we can append the small dataframe to our dataframe to create duplicates for our next exercise.\n",
    "\n",
    "We will assign the combined dataframe to a dataframe named combined_df. You will shortly see why."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17dfcb7267f21c29"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(f'Length of the dataframe: {len(df)}')\n",
    "print(f'Length of the small dataframe: {len(small_df)}')\n",
    "print(f'Length of the combined dataframe is supposed to be: {len(df) + len(small_df)}')\n",
    "combined_df = pd.concat([df, small_df]) # Write to a new dataframe so we preserve\n",
    "print(f'New length of the dataframe: {len(combined_df)}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8161dce9e6fab24",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now look at the head and the tail of the combined dataframe. The head will show the rows from the first dataframe, and the tail will show the rows from the second, smaller one. \n",
    "\n",
    "What do you see?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88a069a73ba4e734"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "combined_df.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33f79d0843dbcc60",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "combined_df.tail()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d47cb459c035200",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "That looks awful is not it? \n",
    "\n",
    "What do you notice? Answer before continue reading "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a878df8183edd53e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "![This is just so you do not see what's written below](https://upload.wikimedia.org/wikipedia/commons/thumb/a/ac/Muhabbet_kuşu_açık_mavi.jpg/1024px-Muhabbet_kuşu_açık_mavi.jpg)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc739b325c4e862d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, small_df did not have an index. id was still a column. Because of that, the combined dataframe has a new id column which shows null for the first part of the dataset as that dataset used id as the index.\n",
    "\n",
    "Secondly, because small_df did not have an index, Pandas assigned a RangeIndex to small_df which goes from 0 to 10000. Hence the index in the tail of the combined dataframe goes from 9995 to 10000.\n",
    "\n",
    "Thirdly, as we renamed content as text and date as created at in the first part of the dataset but did not do the same modifications to the small_df, we now have the same columns with both names and null values. \n",
    "\n",
    "The morale of the story: Always make sure the dataframes you are going to concatanate has the same structure"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a49817d30acf9f5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's introduce the same changes to small_df as the df and then try again"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6263ef529966f4e1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "small_df.set_index('id', inplace=True)\n",
    "small_df.rename(columns = {'date':'created_at', 'content': 'text'}, inplace = True)\n",
    "combined_df = pd.concat([df, small_df])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77ecc95189d35a89",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "combined_df.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54082d4222a34d05",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "combined_df.tail()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88eebeee749dada1",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Identifying and Dropping Duplicates\n",
    "Duplicates may skew your results or may yield in meaningful results. \n",
    "You should consider what type of duplicates may be problematic for your analysis and define & drop duplicates accordingly. \n",
    "\n",
    "We will consider multiple cases of duplicates and learn how to deal with them."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf5724c095e1bef2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1) Duplicate Indexes\n",
    "In Pandas, an index does not have to be unique, but a unique index is strongly recommended for clarity, performance and correctness.\n",
    "\n",
    "On Twitter, ids are serve as index: they are unique and are used as the address of the tweet, e.g., \"1895669466786402519\" in [https://x.com/TheMisterFrog/status/1895669466786402519](https://x.com/TheMisterFrog/status/1895669466786402519) is the tweet id.\n",
    "\n",
    "If your Twitter dataset has rows with the same ids (which we just set as index), either they are the same exact tweets or tweet ids are stored or read incorrectly.\n",
    "\n",
    "We do not have this problem in our dataset. However combined_df has it since we appended a subset of the data, creating many duplicates."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ed4e5124cb246b0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.index.is_unique"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d622020828a9f7c5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "combined_df.index.is_unique"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff459c279c8356d6",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "To drop rows with duplicated indexes, we first, identify the rows where the index is duplicated using `df.index.duplicated(keep='first')` This will create a boolean mask that has True for rows with duplicated indexes **except for their first instance**.\n",
    "Then we return a dataframe where the index is NOT duplicated (using ~) and assign it to itself. Again, we will come to masking in a bit"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "682746516e959b40"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "combined_df = combined_df[~combined_df.index.duplicated(keep='first')]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8679fb2872c52219",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, the combined_df's index will be unique too"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81ee25c26a441652"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "combined_df.index.is_unique"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d1747a7b8fdb55ec",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "And combined_df just became equal to df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55d062cd666d6c33"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "combined_df.equals(df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43ed1979cf5e1f0e",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Duplicate Rows\n",
    "In some cases, the index (e.g., id) may be unique but the values are the same. Interestingly, this is the case in our dataset:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1dba018f20123732"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df[df.duplicated()]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bde01f8a7bfa1bf3",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Seems like a clumsily programmed Twitter bot got a fatal error and tweeted \"@gpt_chatgpt Response exceeds tweet character limit\" 11 times (plus 2 tweets from random people). \n",
    "This is not a big deal for a dataset of 500,000 tweets. But let's drop them anyway.\n",
    "This one is easy, just call `drop_duplicates()`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7bdca9684c469dd4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace = True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c179e255d4813b6e",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "You may also consider the rows in which the same user tweeted the same thing are as duplicates even if they did not tweet those at the same time.\n",
    "Use `subset` to consider such duplicated columns. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "588ade5180c46ad2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df[df.duplicated(subset = ['username', 'text'])] # same user tweets the same text"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74e3d2eda133ab7c",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "It appears that there are 3588 such duplicates. Again not a big number to skew results. But let's drop these too.\n",
    "We again use `drop_duplicates` but this time use a subset.\n",
    "`drop_duplicates` keeps the first instance of a duplicated row as default. you can opt for keeping the last instance with `keep = 'last'` or drop all duplicates with `keep = False`. We will continue keeping the first instance, so no need to put anything"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc8659daf8d73541"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset = ['username', 'text'], inplace = True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78f851d92fd26728",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating and Dropping Columns"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e350529c736d349"
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can create new columns using a simple assignment statement.\n",
    "The following will assign the same value to each for the new column. So, a constant column"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd2b417339dc3bac"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df['year'] = 2023 # every row has the same value"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79eff904edcc4774",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can also use the preexisting columns to create new columns by simple operations"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d385b3431e2d2fce"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df['engagement_count'] = df['like_count'] + df['retweet_count']\n",
    "df['like_retweet_difference'] = df['like_count'] - df['retweet_count']\n",
    "df['like_retweet_ratio'] = df['like_count'] / df['retweet_count']\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8bf50288363f9453",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The year column was a bit useless, let's drop it. \n",
    "Use drop to drop columns"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f40774567d0810a3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.drop(columns = 'year', inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "997b0b41045ccd7",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    ".. or drop multiple columns"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ab8a327d8898a74"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.drop(columns = ['engagement_count', 'like_retweet_difference'], inplace = True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3cd9f817f7e0364b",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can also drop rows by using `df.drop(index = ['....'])` and provide the list of indexes (in our case, ids) of the rows you want to drop. We generally use masking to drop rows though, you will see soon (sorry for the suspense!)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9bec9228d78d464d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Handling Missing Values or NaNs (Not a Number)s\n",
    "In the beginning of the notebook, we handled the bad rows by dropping nas. In some cases, the data is read correctly, the rows are ok. Some values are either missing or there are problems in their computation.\n",
    "\n",
    "In our case, when we compute like_retweet_ratio, we received a lot of NaNs by dividing zero by zero as many tweets have zero likes and zero retweets. \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6789c673596eb7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(f'Number of NaNs in like_retweet_ratio: {len(df[df.like_retweet_ratio.isna()])}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7cbcea39ef8ac9ce",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "One obvious solution is to smooth the values before dividing, e.g., add 1 to like and retweet count and then delete. \n",
    "\n",
    "However, if you instead opt for a solution where you treat tweets with zero likes and retweets, you can instead fill the NaN values with -1 (or leave them as it is)\n",
    "\n",
    "You can use `df.fillna(-1, inplace = True)` to fill all NaNs in the dataframe with -1s. However, it is a better practice to specify the columns where you will fill the NaNs\n",
    "\n",
    "For filling nas in a column, use assignment statement"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59665eda4914d3e2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df['like_retweet_ratio'] = df['like_retweet_ratio'].fillna(-1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9aac680f47f4e1c8",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are other strategies for filling missing values, e.g., filling with the column's mean or median. For time-series data, you can use the value in the previous column (backward fill, `df.fillna(method='bfill')`) or the value in the next column (forward fill, `df.fillna(method='ffill'))`). You can explore these in your free time."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f36d47aa0f062ecd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating Complex Columns\n",
    "You often create new columns for complex analysis or to use as feature for machine learning. Use apply() functions for that.\n",
    "\n",
    "You can define new functions and use them in the apply() function to apply it to a column. The following function identifies and returns a list of hashtags in the tweet."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee8f8919ce51eaae"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def extract_hashtags(text):\n",
    "    hashtags = []\n",
    "    splitted = text.split(' ') # split the text into words using spaces\n",
    "    for word in splitted: # loop through each word\n",
    "        if ((word.startswith('#') and len(word) > 1)): # check if the word starts with # and longer than 1, this means the word is a hashtag\n",
    "            hashtags.append(word) # add the word to the list of hashtags        \n",
    "    return hashtags # return the list of hashtags"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be05b3b4a8989ad5",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now apply extract hashtags function to text using apply() and assign the output to a new column called hashtags."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11ce79c821278cba"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df['hashtags'] = df['text'].apply(extract_hashtags)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb5374b9fcb69bac",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "extract_hashtags is an input to the function apply(), not a function call itself, so you do not call it like `extract_hashtags(text)` and apply() already knows the input is the 'text' column"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20d3d07a26e6bd79"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In cases where the custom function is not that complex, you can use a one-liner \"lambda\" function"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5056a078c38c2535"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df['hashtags'] = df['text'].apply(lambda text: [word for word in text.split(' ') if word.startswith('#') and len(word) > 1]) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1c905bd8e19f64d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "`[word for word in text.split(' ') if word.startswith('#') and len(word) > 1]` is called list-comprehension and one of signature features of Python you should be familiar with or learn it in your free-time."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96ed964cda9c6c30"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Custom function with multiple columns\n",
    "What if you need to use multiple columns as input to the function you provide to apply()? We mix the two approaches we just learn.\n",
    "\n",
    "First, let's say we want to identify \"self-replies\", the tweets which contain the username of their author. This is often the case when users author a Twitter flood / thread.\n",
    "\n",
    "We define a custom function for that:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6b116462e239f5d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def identify_self_replies(text, username):\n",
    "    reply_username = f'@{username}' # replies start with @\n",
    "    return text.startswith(reply_username) # returns true if the text starts with the @username, indicates a self-reply"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2792a120212257e",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will use a lambda function again. \n",
    "However, this time, the input of the lambda function will be the row. Thus, we do not use apply on a particular raw but on the dataframe directly: `df.apply(..`\n",
    "and we will provide axis = 1 as input to the apply() function to tell pandas to apply the function **row-wise**."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9da4c708163a7388"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df['is_self_reply'] = df.apply(lambda row: identify_self_replies(row['text'], row['username']), axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a99202dd433ae3",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Again, we can use one-liners instead of custom functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ba7acdbc3dcfffe"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df['is_self_reply'] = df.apply(lambda row: row['text'].startswith(f'@{row[\"username\"]}'), axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87fba92d7e308506",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see how many self_replies we got:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e950e3bc4509d50"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.is_self_reply.value_counts()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5363a40bd7cf559",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Seems like not many, but that's life :) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "506d20d1243f17b9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This was the part one.\n",
    "The part two will cover:\n",
    "Masking to filter rows\n",
    "Sorting\n",
    "Group by and aggregations\n",
    "Merging and joining\n",
    "Crosstab\n",
    "Covariation and Correlation\n",
    "Basic visualizations such as scatterplot and histogram\n",
    "Writing data\n",
    "Handling dates\n",
    "Sampling\n",
    "Reshaping and Pivoting\n",
    "Read JSONs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0d96920ff551593"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "49518e4b92fa7d8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
