{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91bdf2098d418109",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Introduction to Data Analysis using Pandas\n",
    "\n",
    "This is part two of simple data analysis using pandas. It consists of more advanced topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4b3bd6ce7947d59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T13:14:09.990016Z",
     "start_time": "2025-05-21T13:14:04.508761Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from helpers.analysis_preprocessor import preprocess # imports the preprocess from helpers/analysis_preprocessor.py \n",
    "df = pd.read_csv('data/twitter_data_chatgpt.csv.bz2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beef8fead2c68972",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Preprocessing.. again.\n",
    "Because we reloaded the data, we need to do the same preprocessing again. A good practice is to define a preprocessing function that runs through all the steps so you can just call preprocess(df). You can also add additional steps later. We added this preprocess functions to helpers/analysis_preprocessor.py  and imported it in the previous block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a84d7163135dcb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T13:14:20.130527Z",
     "start_time": "2025-05-21T13:14:17.772296Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = preprocess(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b6c9c715cc9add",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Filtering Data\n",
    "Filtering is the process of subsetting a dataset based on a logical condition.\n",
    "It allows you to zoom in on relevant data: analyze them, visualize them, discard them or do certain manipulations on them much. You will be using it a lot. \n",
    "\n",
    "Filtering is done by a list of Boolean values that has the same size as the dataframe you will filter on like `[True, True, False....]`. It is called a (boolean) mask. You use the mask `df[mask]` to return a new Dataframe with just the rows for which the mask has a True value.\n",
    "\n",
    "For instance, let's build a mask that has values all set to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ad1b69b24f5f590",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "mask = [True for value in range(0, df.shape[0])] # this creates a list of 500,000 Trues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461d85ec48932d3b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "(Note that this code is inefficient and you should create this list using `mask = np.ones(df.shape[0], dtype=bool)` but that's not important for now)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ad60eef38b50e7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Then use this on our dataframe. We get the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e74904da2dfb90",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "df[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905299eb03ee3e9f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's try a mask where only the first value is True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aae6bb13fe42e0",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "mask = [True] + [False for value in range(1, df.shape[0])] # this creates a list of 500,000 Trues\n",
    "df[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278ce4b9",
   "metadata": {},
   "source": [
    "You of course won't create masks manually but by applying some logic to the data itself.\n",
    "\n",
    "Let's filter the tweets that acquired at least 1 engagement (remember our newly created column engagement counts which is likes plus retweets?)\n",
    "For that we need to create a mask that has True for all the rows where the engagement_count is greater than 0. (or more than or equal to 1)\n",
    "This is simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296009c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['engagement_count'] > 0 # df['engagement_count'] >= 1 works too"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec4b58c",
   "metadata": {},
   "source": [
    "Than we just put this mask into brackets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cac955",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['engagement_count'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d543571e",
   "metadata": {},
   "source": [
    "You can filter by multiple conditions. \n",
    "You have to wrap each condition in parentheses and put logical operators like & (AND), | (OR).\n",
    "\n",
    "For instance, suppose we want tweets that have some engagements but they should have AT LEAST ONE retweet so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3d4178",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['engagement_count'] >= 1 & df['retweet_count'] >= 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b29c5e",
   "metadata": {},
   "source": [
    "Wait, what the hell happened? I told you to wrap each condition in parenthesis remember?\n",
    "\n",
    "When you omit parentheses, Pandas evaluates the expression like `df['engagement_count'] >= (1 & df['retweet_count']) >= 1` because comparison operators such as >= have higher precedence than bitwise operators like &. So, we use parentheses to explicitly control the order of evaluation—just like in math—ensuring that each condition is evaluated before they are combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d85c060",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['engagement_count'] >= 1) & (df['retweet_count'] >= 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06136269",
   "metadata": {},
   "source": [
    "You can use the query method for complex conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bbecd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"engagement_count >= 1 and retweet_count >= 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b642d61b",
   "metadata": {},
   "source": [
    "Note that you can always define variables which are masks or queries for readability and to use them later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dfbbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "at_least_one_rt_and_engagement = (df['engagement_count'] >= 1) & (df['retweet_count'] >= 1)\n",
    "df[at_least_one_rt_and_engagement]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f39a1e2",
   "metadata": {},
   "source": [
    "You can use string methods to filter string columns in a DataFrame. To do this, we need to use the .str accessor (..to tell Pandas that we are not applying the string method to the whole column but each string value in the column individual) (Pandas are not the smartest that's why they face extinction!)\n",
    "\n",
    "For instance, let's filter the tweets that start with '@', these are replies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63485b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.text.startswith('@')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5afe82",
   "metadata": {},
   "source": [
    "This did not work, because we needed to use the .str accessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f9d7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.text.str.startswith('@')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db355bc5",
   "metadata": {},
   "source": [
    "Tweets that contain a link:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9948b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.text.str.contains('https://')) | (df.text.str.contains('http://'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef898353",
   "metadata": {},
   "source": [
    "Pandas knows how to do element-wise comparison so you do not need to use .str for string comparisons\n",
    "\n",
    "Let's find the tweets authored by me! Omg I'm busted :o "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360605e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.username == 'tugrulcanelmas']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a5baf5",
   "metadata": {},
   "source": [
    "What if we want to collect tweets from multiple users and thus, have to do multiple comparisons? We can go like `(df.username == 'tugrulcanelmas') | (df.username == 'elonmusk') | (....)` but that is just too much code. \n",
    "\n",
    "It's better to create a list of usernames and then use `isin`.\n",
    "Let's get tweets from SICSS Organizers: @tugrulcanelmas, @Walid_Magdy, 'Bjoernross, @TVGsociologist, @ZeerakTalat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b38f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "SICSS_list = ['tugrulcanelmas', 'Walid_Magdy', 'Bjoernross', 'TVGsociologist', 'ZeerakTalat']\n",
    "df[df.username.isin(SICSS_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972593e9",
   "metadata": {},
   "source": [
    "Seems like only Tuğrulcan and Walid tweeted, Zee migrated to BlueSky, Tod forgot he had an account. That's life.\n",
    "\n",
    "Note that you can use the `isin` for other dtypes like integers but you usually don't."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11036460",
   "metadata": {},
   "source": [
    "### Filter & Modify\n",
    "You can create new columns by filtering on existing data.\n",
    "For that, you need to use `.loc` to access columns and then assign new values.\n",
    "The first time you do this on undefined, pandas will create a new column. However, all unassigned values will be set to null. Therefore **you need to assign a default value first**\n",
    "\n",
    "Let's say we want to create a new column named \"tweet_type\" that indicates whether if the tweet is a __retweet__, a __reply__ or a regular, plain __tweet__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "084751d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column with default value\n",
    "df['tweet_type'] = 'tweet'\n",
    "\n",
    "# Now we can use .loc to assign new values\n",
    "# We use the .str.startswith() method to check if the text starts with 'RT' for retweets, \n",
    "df.loc[df['text'].str.startswith('RT'), 'tweet_type'] = 'retweet'\n",
    "# We use the .str.startswith() method to check if the text starts with '@' for replies\n",
    "df.loc[df['text'].str.startswith('@'), 'tweet_type'] = 'reply'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ec76c5",
   "metadata": {},
   "source": [
    "Note that .loc accessor is to access rows. Remember the first part where we use ids to access the corresponding rows? (e.g., `df.loc[1641213003260633088]`)\n",
    "In this example, what we did was to indicate the rows that satisfy a condition (e.g., `df.loc[df['text'].str.startswith('@')]`), instead of explicitly giving them a list of ids. \n",
    "We then selected a column and assign it a new value, for the rows that we accessed based on that condition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68da524e",
   "metadata": {},
   "source": [
    "Exercise: Remember Analysis Part 1. How could we achieve the same result using apply()?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1924a35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet_type_byapply'] = ...\n",
    "# check\n",
    "df['tweet_type'] == df['tweet_type_byapply']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be589d9d",
   "metadata": {},
   "source": [
    "# Sorting\n",
    "Sorting can be done in two ways: sort index or sort values. \n",
    "Sorting by index is faster but it sorts the index itself. \n",
    "Sorting by values uses the values (i.e., the columns) in the dataframe to sort the data. It is slower.\n",
    "\n",
    "In our case, the data was sorted according to date but in the reverse chronological order for some reason.. Let's fix this.\n",
    "We can sort the data by the date column or the index, which was the id. \n",
    "Since ids are ascending by date, they provide the same result as sorting by date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f46382",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_index(inplace = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8d3154",
   "metadata": {},
   "source": [
    "To sort on a specific column we follow the template `sort_values(by = 'COLUMN TO SORT',)`. \n",
    "\n",
    "Note that both sort index and sort values sort in ascending order by default. But you can change it by setting ascending = False. \n",
    "\n",
    "Let's sort tweets according to the number of engagements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec28740f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by = 'engagement_count', inplace = True, ascending = False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3c2736",
   "metadata": {},
   "source": [
    "# Aggregation & Group By\n",
    "In data analysis, aggregation refers to the process of summarizing or condensing data. Instead of analyzing individual rows, aggregation allows us to see patterns across groups — e.g., the average number of engagements each user receives or the total number of tweets per day. It helps reveal hidden insights in the data, such as emergent events indicated by sudden spikes in tweet volume.\n",
    "\n",
    "Basic Aggregation Functions:\n",
    "- count(): Count non-null values\n",
    "- sum(): Sum of values\n",
    "- mean(): Average of values\n",
    "- min()/max(): Minimum/maximum values\n",
    "- std(): Standard deviation\n",
    "- size(): Number of rows in a **group** (**including null values**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1adbac",
   "metadata": {},
   "source": [
    "For instance, let's see the tweet with the highest number of engagements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7033ac9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.engagement_count == df['engagement_count'].max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f26982",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca41f982",
   "metadata": {},
   "source": [
    "## Group By Operation: \n",
    "The groupby() function allows you to split data into groups and perform operations on each group.\n",
    "\n",
    "`df.groupby('column_name')` gives you a DataFrameGroupBy object - a special intermediate object that represents the grouping operation. It is meaningless by itself, you need to perform an operation: aggregation, iteration or filtering.\n",
    "\n",
    "Use the following template for aggregation:\n",
    "`df.groupby('column_name').aggregation_function()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d684497e",
   "metadata": {},
   "source": [
    "For instance, the number of rows for each user (which means the number of tweets each user posted), and then sort the values to see who babbled the most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008c031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('username').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58d49ac",
   "metadata": {},
   "source": [
    "Add sort_values to see who babbled the most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3687089",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('username').size().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06fd02e",
   "metadata": {},
   "source": [
    "The total number of engagement each user "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a3acb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('username')['engagement_count'].sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f795c102",
   "metadata": {},
   "source": [
    "You can have multiple aggregation functions using `agg()``\n",
    "Let's compute the total and average number of engagements each user receives: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8a0daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('username').agg({'engagement_count': ['sum', 'mean']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b2a954",
   "metadata": {},
   "source": [
    "You can also aggregate by multiple columns. \n",
    "\n",
    "For instance, the total number of engagement each user receives in each day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3681304",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['username', 'created_at'])['engagement_count'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ca32d9",
   "metadata": {},
   "source": [
    "Notice that 000Dave tweeted multiple dates, starting in 16/01/2023 and received different number of engagement counts each day. Good job, I guess?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6670c0",
   "metadata": {},
   "source": [
    "You can also define and apply a custom function after groupby.\n",
    "\n",
    "For instance, let's get the ratio of total likes to total retweets received by each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deee6f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def like_retweet_ratio(x):\n",
    "    return x['like_count'].sum() / (x['retweet_count'].sum() + 1) # smoothing the denominator to avoid division by zero\n",
    "\n",
    "df.groupby('username').apply(like_retweet_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368468a8",
   "metadata": {},
   "source": [
    "You can iterate for each group if you need to. You usually better off if you use aggregation functions. However, in some cases you may need to run complex logic individually on each group's DataFrame. Or you may need to iterate for visualization purposes\n",
    "\n",
    "Let's group by the tweet type and describe each group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1164a43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet_type, group in df.groupby('tweet_type'):\n",
    "    print(f\"Tweet type: {tweet_type}\")\n",
    "    group_size = group.shape[0]\n",
    "    max_engagement = group['engagement_count'].max()\n",
    "    mean_engagement = group['engagement_count'].mean()\n",
    "    print(f\"Group size: {group_size}\")\n",
    "    print(f\"Max engagement: {max_engagement}\")\n",
    "    print(f\"Mean engagement: {mean_engagement}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b579683",
   "metadata": {},
   "source": [
    "The mean engagement for regular tweets is the highest because there are many AI Influencers out there farming engagements with crappy tutorials. Pity. Anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9abe70",
   "metadata": {},
   "source": [
    "Lastly, you can filter group based on a condition.\n",
    "For instance, keep the tweets from the users who received more than 1000 engagements in total stays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cd0ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('username').filter(lambda x: x['engagement_count'].sum() > 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b755982",
   "metadata": {},
   "source": [
    "Note that this grouped the DataFrame by username, filtered out the groups that don't meet the condition, and then merged the remaining groups back into a single DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe24119",
   "metadata": {},
   "source": [
    "# Joining & Merging \n",
    "In Part 1, we learnt how to concatanate two dataframes. This was simple, just two dataframes stacked on top of each other. However, remember that we had problems when columns differed - we had to align them.\n",
    "\n",
    "We concatanated vertically: we added the rows of one dataframe to another. What if we wanted to concatanate horizontally? That is, what if we wanted to add the **columns**  of a dataframe? Well, just taking two dataframes and sticking with pd.concat() still works. But in such a scenario, we often need to align the two dataframe so the data in the two dataframes will be consistent. Moreover, we need to multiply the rows in one of the dataframe to match the other.\n",
    "\n",
    "Consider the case where we collect additional data about Twitter users like their popularity so each row in our new dataframe consists of a username and their popularity. How can we append this data to our existing data?\n",
    "\n",
    "That's where `join and merge` operations come in. They unite the dataframes. They empower them. They make us stand!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac70792",
   "metadata": {},
   "source": [
    "Let's suppose we have a user dataframe `user_df` that holds additional data about users. We don't have such a dataframe so I'll create one using aggregation. The dataframe will consists of usernames and their popularity. The total engagement count the users receive will be a proxy for user popularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7889f026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the total engagement count for each user\n",
    "user_df = df.groupby('username')['engagement_count'].sum().sort_values(ascending = False)\n",
    "\n",
    "# Create a new dataframe with the username and popularity\n",
    "user_df = pd.DataFrame({'username': user_df.index, 'popularity': user_df.values})\n",
    "user_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6a033c",
   "metadata": {},
   "source": [
    "Now we want to merge this with our original dataframe to incorporate popularity data. We use the following template: `first_dataframe.merge(second_dataframe, on = 'column_to_merge')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be0ef02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.merge(user_df, on = 'username') # note that we did not use inplace = True yet so it did not change the original dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f270f133",
   "metadata": {},
   "source": [
    "In many cases, the same data may have different column names across different dataframes. My recommendation is to rename columns so they match before performing a merge. However, if you prefer not to rename columns or want to keep both columns, you can specify different columns to merge on during the merge operation.\n",
    "\n",
    "For example, suppose the username column in user_df is named screen_name instead of username (which is how the Twitter API actually names usernames, so you'll often encounter this when analyzing Twitter data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d3cc617",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df.rename(columns = {'username': 'screen_name'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45513900",
   "metadata": {},
   "source": [
    "When merging dataframes with different column names, we specify which columns to use from each dataframe. The first dataframe is called \"left\" and the second is called \"right\". We use the parameters \"left_on\" to specify the column from the left dataframe and \"right_on\" to specify the column from the right dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb54ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.merge(user_df, left_on = 'username', right_on = 'screen_name')\n",
    "# Note that we did not use inplace = True yet so it did not change the original dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088b6fcb",
   "metadata": {},
   "source": [
    "Notice that username and screen_name has the same data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059f2a29",
   "metadata": {},
   "source": [
    "## Join\n",
    "Join is the special case of merge where you merge on the index of the dataframes instead of columns. It's more efficient than merge\n",
    "\n",
    "The basic syntax is: `dataframe1.join(dataframe2)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feb58b7",
   "metadata": {},
   "source": [
    "Let's suppose we have additional data related to tweets, their popularity. Since I do not have such data, I will create a fake one by setting the engagement count as tweet popularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb800a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_df = df[['engagement_count']]\n",
    "fake_df.rename(columns={'engagement_count': 'popularity'}, inplace=True)\n",
    "fake_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01044119",
   "metadata": {},
   "source": [
    "Since both df's and fake_df's index is id, they can be joined together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e733ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.join(fake_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d50b6ac",
   "metadata": {},
   "source": [
    "What if we joined fake_df, df and then merged with user_df?    \n",
    "(temporarily dropping some columns so you can see the output better)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f953f51",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a3911b",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporarily_drop_those_columns_for_readability = ['hashtags', 'tweet_type', 'engagement_count', 'like_count', 'retweet_count', 'created_at', 'text']\n",
    "df.join(fake_df).merge(user_df, left_on = 'username', right_on = 'screen_name').drop(columns = temporarily_drop_those_columns_for_readability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fa2967",
   "metadata": {},
   "source": [
    "If you would look at the rightmost part of the merged dataframe, you will see `\"popularity_x\"` and `\"popularity_y\"` and they are different! One indicates the tweet popularity and the other indicates user popularity! \n",
    "\n",
    "Pandas, realizing your mistake, hastily renamed them by appending suffixes _x and _y. _x indicates left and _y indicates right. You can explicitly indicate suffixes while merging. But my recommendation is to properly rename them before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04d84f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.join(fake_df).merge(user_df, left_on = 'username', right_on = 'screen_name', suffixes = ['_tweet', '_user']).drop(columns=temporarily_drop_those_columns_for_readability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f915b2",
   "metadata": {},
   "source": [
    "Now we have more descriptive suffixes, but again:\n",
    "\n",
    "**Properly rename your columns before join and merge operations!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8741c9",
   "metadata": {},
   "source": [
    "## Left, Right, Inner, Outer.. what the heck?\n",
    "In many (annoying) cases, your data may not match up as expected. One dataframe might be missing information, while another might contain more data than anticipated. For instance, your original dataframe consists of tweets collected in 2023. You collect additional user data from Twitter data from X in 2025. You discover that some users have been suspended and you could not collect their data. Your user dataframe is incomplete.\n",
    "\n",
    "You have several options:\n",
    "1. Keep the original dataframe as it is, but leave the user data as null for users whose information couldn’t be retrieved.\n",
    "2. Remove the tweets from the users with incomplete data, so you only keep the data with non-null values\n",
    "\n",
    "The first approach is called an **outer** join/merge, while the second approach is called **inner** join/merge. The following image sums up very well:\n",
    "\n",
    "![joins](./data/joins.jpg)\n",
    "\n",
    "- An inner join returns only the rows where both dataframes have matching keys — essentially a set intersection.\n",
    "- A left outer join keeps all rows from the left dataframe, even if there's no matching row in the right dataframe.\n",
    "- A right outer join does the opposite, keeping all rows from the right dataframe.\n",
    "- A full outer join includes all rows from both dataframes, with unmatched columns filled with nulls.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d743dd38",
   "metadata": {},
   "source": [
    "Let's practice with the hypothetical scenario we just explained: suppose the user_df is incomplete. I will make this by slicing the user_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090cd5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df_incomplete = user_df[0:len(user_df)//2] \n",
    "user_df_incomplete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08adc7fb",
   "metadata": {},
   "source": [
    "You determine the join/merge type by the `\"how\"` argument. The options are `inner, left, right, outer`. The default value is `inner`\n",
    "\n",
    "Inspect the output of the inner join and left join. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54979464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inner\n",
    "df.merge(user_df_incomplete, how = 'inner', left_on = 'username', right_on = 'screen_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80e80fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# left outer\n",
    "df.merge(user_df_incomplete, how = 'left', left_on = 'username', right_on = 'screen_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573aa984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# right outer \n",
    "df.merge(user_df_incomplete, how = 'right', left_on = 'username', right_on = 'screen_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba69355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full outer \n",
    "df.merge(user_df_incomplete, how = 'outer', left_on = 'username', right_on = 'screen_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb86b297",
   "metadata": {},
   "source": [
    "# Reshaping Data\n",
    "\n",
    "## Exploding Lists\n",
    "We have extracted the hashtags in each tweet and put it to the `\"hashtags\"` column. Notice that it contains a list as a single tweet may contain multiple hashtags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e14025",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7eb0bfa",
   "metadata": {},
   "source": [
    "You can access the elements inside list columns using apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb953136",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hashtags'].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0cc3f9",
   "metadata": {},
   "source": [
    "... and you get your first error because there are tweets **without any hashtag**, which means their hashtags column is an empty list (hence list index out of range error)\n",
    "\n",
    "A work around is that returning None (or a dummy value like \"NOHASHTAG\") for empty lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b255a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hashtags'].apply(lambda x: x[0] if len(x) > 0 else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59aa5f1b",
   "metadata": {},
   "source": [
    "What you will do with this hashtag column is up to you. You can convert each hashtag into a new column (e.g., `hashtag1`, `hashtag2`, `hashtag3`...) or concatante into a single string. \n",
    "\n",
    "A very nice trick is \"exploding\" the list: flattening the list by keeping only a single hashtag in the column but duplicating the rows for the tweets with multiple hashtags. This is useful when you later group by or filter on some hashtags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50db8f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the DataFrame on the hashtags column\n",
    "exploded_df = df.explode('hashtags')\n",
    "\n",
    "# Display the first few rows to verify the explosion\n",
    "print(\"Exploded DataFrame:\")\n",
    "print(exploded_df[['username', 'hashtags']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895d9eb3",
   "metadata": {},
   "source": [
    "Notice that johnvianny used 3 hashtags in the tweet with id `1613755402608381952` so we have three records of the same tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6e7d26",
   "metadata": {},
   "source": [
    "We now can do a simple group by operation to show the number of times each user used a certain hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e54920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by username and hashtags, then count occurrences\n",
    "hashtag_usage = exploded_df.groupby(['username', 'hashtags']).size().reset_index(name='count')\n",
    "\n",
    "# Sort by count in descending order to see most frequent hashtag usage\n",
    "hashtag_usage = hashtag_usage.sort_values('count', ascending=False)\n",
    "\n",
    "# Display the top 10 most frequent hashtag usages\n",
    "print(\"Top 10 most frequent hashtag usages by user:\")\n",
    "print(hashtag_usage.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5eb3d83",
   "metadata": {},
   "source": [
    "MidJourneyAI_ seems to have used the same set of 5 hashtags 1221. You may be thinking that the code is wrong (like me). Well, no, MidJourneyAI_ appends the same hashtags to every tweets that's why the results are like this :))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934644da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.username=='MidJourneyAI_', 'hashtags']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e375f093",
   "metadata": {},
   "source": [
    "## Pivoting\n",
    "The hashtag_usage dataframe is currently structured like `(user1,hashtag1,value),(user1,hashtag2,value),(user2,hashtag1,value)...` \n",
    "You see it looks like a flat matrix.       \n",
    "You can convert it to a matrix where the rows represent the users, columns represent the hashtags and each cell indicate the usage (count). This is called pivoting. This is useful for visualization and also when applying techniques like clustering and dimensionality reduction that you will see later.\n",
    "\n",
    "In our example there are too many users and hashtags so I will pivot on only the first 50 rows of the hashtag usage dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a71398e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivotted = hashtag_usage.head(50).pivot(index='username', columns= 'hashtags', values='count')\n",
    "pivotted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b875410b",
   "metadata": {},
   "source": [
    "You can reverse pivotting by \"melting\". Do not forget to drop rows with null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78bddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_df = pivotted.reset_index().melt(id_vars='username', var_name='hashtag', value_name='count')\n",
    "melted_df = melted_df.dropna().sort_values(by = 'count', ascending=False)\n",
    "melted_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f38953",
   "metadata": {},
   "source": [
    "# Saving Data\n",
    "You can easily save your data to a csv by `.to_csv(filename)`, e.g., `df.to_csv('tweets.csv')`\n",
    "\n",
    "However, I have some tips to avoid frustration later\n",
    "\n",
    "- **Named index**: If you're using an index and it has a name, you're good to go. For example, if you’ve set the index to 'id' (i.e., `df.set_index('id')`), then the index will be saved properly, and no extra work is needed.\n",
    "- **Unnamed index:** If you’ve assigned a custom index (e.g., `df.index = ...`) without setting a name, make sure to add one while saving: `df.to_csv(filename, index_label='your_index_name')`. Otherwise, the index column will just be labeled \"index\" in the CSV.\n",
    "- **No index / Default Pandas index:** If you are NOT using an index, which means you are using the index automatically assigned by pandas, use `df.to_csv(filename, index = False)` to avoid pandas saving its auto-generated index. If you do not do this, you will add an extra column everytime you resave the same file.\n",
    "- **Annoying Characters:** If you have string fields, I recommend you to **strip \"\\n\"s and \"\\r\"s** from them and then save. They often break formatting in CSV files and create more problems then they solve.\n",
    "- **Complex Fields:** If you have fields containing lists, dictionaries, or other complex data, save in JSON format (.to_json) rather than CSV. CSV isn’t designed for nested structures and will make reloading or parsing those fields harder.\n",
    "- ** Save as Records:** If saving as JSON, pandas defaults to a data structure in which the whole data is packed in a single JSON object. This is speed and memory efficient but it is a pain if you want to read the data line by line. Thus, save to json by setting `orient = records` so each row will correspond to a separate json object. If you do this, you need to read them as `pd.read_json(filename, lines = True) later.\n",
    "- **Compress:** Compress your csvs and jsons so they will take less space, using `compression = 'bz2'`\n",
    "\n",
    "and the most important tip:\n",
    "\n",
    "**NEVER OVERWRITE THE ORIGINAL DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "194012c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df.text.apply(lambda x: x.replace('\\n', ' ').replace('\\r', ' '))\n",
    "df.to_json('twitter_data_chatgpt_v2.json.bz2', orient='records', compression = 'bz2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
