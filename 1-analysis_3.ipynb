{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading JSON\n",
    "We saved the json as records (i.e., each row is a separate json object) remember?\n",
    "So we set `lines = True` to signal this to pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>username</th>\n",
       "      <th>like_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>engagement_count</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1610535734758219778</th>\n",
       "      <td>2023-01-04 07:16:56+00:00</td>\n",
       "      <td>I used chat gpt to get gym workout program and...</td>\n",
       "      <td>pnik91</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1610535786017091584</th>\n",
       "      <td>2023-01-04 07:17:08+00:00</td>\n",
       "      <td>I'm quite amazed by Chat GPT. A really promisi...</td>\n",
       "      <td>manumurali369</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1610535837363486720</th>\n",
       "      <td>2023-01-04 07:17:20+00:00</td>\n",
       "      <td>all my twitter feed is about ChatGPT and @Open...</td>\n",
       "      <td>mcp350</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[#ChatGPT]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1610535961670172674</th>\n",
       "      <td>2023-01-04 07:17:50+00:00</td>\n",
       "      <td>#ChatGPT   So much #Censorship.  Never trust a...</td>\n",
       "      <td>TryingToOffend</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[#ChatGPT, #Censorship.\\n\\nNever]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1610536038094757888</th>\n",
       "      <td>2023-01-04 07:18:08+00:00</td>\n",
       "      <td>@GoogleAI #LAMDA Versus @OpenAI #ChatGPT ?! Wh...</td>\n",
       "      <td>Pup_In_Cup</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[#LAMDA, #ChatGPT]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1641213003260633088</th>\n",
       "      <td>2023-03-29 22:57:26+00:00</td>\n",
       "      <td>Most people haven't heard of Chat GPT yet. Fir...</td>\n",
       "      <td>nikocosmonaut</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1641213110915571715</th>\n",
       "      <td>2023-03-29 22:57:52+00:00</td>\n",
       "      <td>AI muses: \"In the court of life, we must all f...</td>\n",
       "      <td>ChatGPT_Thinks</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[#OutOfContextAI, #AILifeLessons, #ChatGPT]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1641213115684536323</th>\n",
       "      <td>2023-03-29 22:57:53+00:00</td>\n",
       "      <td>https://t.co/FjJSprt0te - Chat with any PDF! C...</td>\n",
       "      <td>yjleon1976</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[#chatpdf, #ChatGPT]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1641213218520481805</th>\n",
       "      <td>2023-03-29 22:58:18+00:00</td>\n",
       "      <td>@MecoleHardman4 Chat GPT says itâ€™s 15. ðŸ˜‚</td>\n",
       "      <td>AmyLouWho321</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1641213230730051584</th>\n",
       "      <td>2023-03-29 22:58:21+00:00</td>\n",
       "      <td>Free AI marketing and automation tools, strate...</td>\n",
       "      <td>RealProfitPros</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[#ChatGPT]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>499961 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   created_at  \\\n",
       "id                                              \n",
       "1610535734758219778 2023-01-04 07:16:56+00:00   \n",
       "1610535786017091584 2023-01-04 07:17:08+00:00   \n",
       "1610535837363486720 2023-01-04 07:17:20+00:00   \n",
       "1610535961670172674 2023-01-04 07:17:50+00:00   \n",
       "1610536038094757888 2023-01-04 07:18:08+00:00   \n",
       "...                                       ...   \n",
       "1641213003260633088 2023-03-29 22:57:26+00:00   \n",
       "1641213110915571715 2023-03-29 22:57:52+00:00   \n",
       "1641213115684536323 2023-03-29 22:57:53+00:00   \n",
       "1641213218520481805 2023-03-29 22:58:18+00:00   \n",
       "1641213230730051584 2023-03-29 22:58:21+00:00   \n",
       "\n",
       "                                                                  text  \\\n",
       "id                                                                       \n",
       "1610535734758219778  I used chat gpt to get gym workout program and...   \n",
       "1610535786017091584  I'm quite amazed by Chat GPT. A really promisi...   \n",
       "1610535837363486720  all my twitter feed is about ChatGPT and @Open...   \n",
       "1610535961670172674  #ChatGPT   So much #Censorship.  Never trust a...   \n",
       "1610536038094757888  @GoogleAI #LAMDA Versus @OpenAI #ChatGPT ?! Wh...   \n",
       "...                                                                ...   \n",
       "1641213003260633088  Most people haven't heard of Chat GPT yet. Fir...   \n",
       "1641213110915571715  AI muses: \"In the court of life, we must all f...   \n",
       "1641213115684536323  https://t.co/FjJSprt0te - Chat with any PDF! C...   \n",
       "1641213218520481805           @MecoleHardman4 Chat GPT says itâ€™s 15. ðŸ˜‚   \n",
       "1641213230730051584  Free AI marketing and automation tools, strate...   \n",
       "\n",
       "                           username  like_count  retweet_count  \\\n",
       "id                                                               \n",
       "1610535734758219778          pnik91           0              0   \n",
       "1610535786017091584   manumurali369           1              0   \n",
       "1610535837363486720          mcp350           3              1   \n",
       "1610535961670172674  TryingToOffend           2              0   \n",
       "1610536038094757888      Pup_In_Cup           1              0   \n",
       "...                             ...         ...            ...   \n",
       "1641213003260633088   nikocosmonaut           0              0   \n",
       "1641213110915571715  ChatGPT_Thinks           0              0   \n",
       "1641213115684536323      yjleon1976           0              0   \n",
       "1641213218520481805    AmyLouWho321           0              0   \n",
       "1641213230730051584  RealProfitPros           0              0   \n",
       "\n",
       "                     engagement_count  \\\n",
       "id                                      \n",
       "1610535734758219778                 0   \n",
       "1610535786017091584                 1   \n",
       "1610535837363486720                 4   \n",
       "1610535961670172674                 2   \n",
       "1610536038094757888                 1   \n",
       "...                               ...   \n",
       "1641213003260633088                 0   \n",
       "1641213110915571715                 0   \n",
       "1641213115684536323                 0   \n",
       "1641213218520481805                 0   \n",
       "1641213230730051584                 0   \n",
       "\n",
       "                                                        hashtags  \n",
       "id                                                                \n",
       "1610535734758219778                                           []  \n",
       "1610535786017091584                                           []  \n",
       "1610535837363486720                                   [#ChatGPT]  \n",
       "1610535961670172674            [#ChatGPT, #Censorship.\\n\\nNever]  \n",
       "1610536038094757888                           [#LAMDA, #ChatGPT]  \n",
       "...                                                          ...  \n",
       "1641213003260633088                                           []  \n",
       "1641213110915571715  [#OutOfContextAI, #AILifeLessons, #ChatGPT]  \n",
       "1641213115684536323                         [#chatpdf, #ChatGPT]  \n",
       "1641213218520481805                                           []  \n",
       "1641213230730051584                                   [#ChatGPT]  \n",
       "\n",
       "[499961 rows x 7 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: the following my take over a minute\n",
    "try:\n",
    "    df = pd.read_json('data/twitter_data_chatgpt_v2.json.bz2', lines = True)\n",
    "    df.set_index('id', inplace=True)\n",
    "except:\n",
    "    print('You did not run the previous notebook! Reading the csv')\n",
    "    df = pd.read_csv('data/twitter_data_chatgpt.csv.bz2')\n",
    "    from helpers.analysis_preprocessor import preprocess # imports the preprocess from helpers/analysis_preprocessor.py \n",
    "    df = preprocess(df)\n",
    "\n",
    "df.sort_index(inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling dates\n",
    "Pandas uses the datetime64 data type to store date-time information efficiently. It provides rich functionality through .dt accessor and DatetimeIndex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a column that represent date is in a correct and Pandas-recognizable format, converting that column to a datetime is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['created_at'] = pd.to_datetime(df['created_at'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some data sources include timezone information in their date-time fields. For instance, Twitter/X (as well as Reddit and YouTube) store datetime data in UTC (which is a fancy name for GMT). \n",
    "\n",
    "This means that if a user tweets from Japan at 11:00 AM Japan Standard Time, the timestamp will appear as 3:00 AM UTCâ€”which can be confusing. If you're working with data from a specific country or region, it's a good idea to convert the timestamps to the local timezone using `tz_convert`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"created_at\"].dt.tz_convert(\"Asia/Tokyo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use .dt accessor to access each date component: `dt.date, dt.year, dt.month, dt.day, dt.hour, dt.day_name()`\n",
    "\n",
    "Let's create new columns to represent the month and the day of the week, and hour of the day each tweet was created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['month'] = df['created_at'].dt.month\n",
    "df['day_of_week'] = df['created_at'].dt.day_name()\n",
    "df['hour'] = df['created_at'].dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can filter on datetime columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter tweets from January 2023\n",
    "jan_2023_tweets = df[df['created_at'].dt.month == 1]\n",
    "print(f\"Number of tweets from January: {len(jan_2023_tweets)}\")\n",
    "\n",
    "# Filter tweets from weekends\n",
    "weekend_tweets = df[df['day_of_week'].isin(['Saturday', 'Sunday'])]\n",
    "print(f\"Number of tweets from weekends: {len(weekend_tweets)}\")\n",
    "\n",
    "# Filter tweets from business hours (9 AM to 5 PM)\n",
    "business_hours_tweets = df[(df['hour'] >= 9) & (df['hour'] <= 17)]\n",
    "print(f\"Number of tweets during business hours: {len(business_hours_tweets)}\")\n",
    "\n",
    "#Â Filter tweets after 1st of March 2023\n",
    "after_first_of_march = df[df['created_at'] > '2023-03-01']\n",
    "print(f\"Number of tweets after the first of March: {len(after_first_of_march)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can groupby a specific date component, e.g., group by the name of the date and show the size of each group, which shows the number of tweets posted in each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.groupby(df.created_at.dt.day_name()).size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can compute the time difference between the datetime and a predetermined date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.created_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Calculate time difference between tweets and a reference date\n",
    "reference_date = pd.Timestamp('2023-01-01 00:00:00+00:00')\n",
    "print('Time passed since 2023 new years:')\n",
    "df['created_at'] - reference_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can subtract between consecutive rows using `diff()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['created_at'].diff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling\n",
    "Resampling is changing the frequency of your time series data. It is especially useful when your data is irregular, which is the case with our Twitter dataset. Tweets are stored as they are posted, and there is no guarantee that we will have at least one tweet for every day, hour, or a second (probably for Twitter we would have, but think about TruthSocial).\n",
    "\n",
    "You resample when you are aggregate the data over time, e.g., the count of tweets per day, hour, per 3 hours etc. Thus, first, you reshape the data so that the index will be the datetime and the values will show some aggregated statistics such as count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Set the datetime column as index and return a new dataframe\n",
    "df_time = df.set_index('created_at')\n",
    "\n",
    "# Resample data to different time frequencies\n",
    "# 'D' (daily), 'W' (weekly), 'M' (monthly), 'H' (hourly)\n",
    "daily_tweets = df_time.resample('D').size()\n",
    "weekly_tweets = df_time.resample('W').size()\n",
    "monthly_tweets = df_time.resample('M').size()\n",
    "\n",
    "# Print the resampled data\n",
    "print(\"Daily tweet counts:\")\n",
    "print(daily_tweets.head())\n",
    "print(\"\\nWeekly tweet counts:\")\n",
    "print(weekly_tweets.head())\n",
    "print(\"\\nMonthly tweet counts:\")\n",
    "print(monthly_tweets.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, more custom frequencies and different aggregations over different columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-hour intervals\n",
    "two_hour_tweets = df_time.resample('2H')['retweet_count'].mean()\n",
    "\n",
    "# 3-day intervals\n",
    "three_day_tweets = df_time.resample('3D')['like_count'].sum()\n",
    "\n",
    "# Business days only (Monday-Friday)\n",
    "business_day_tweets = df_time.resample('B').size()\n",
    "\n",
    "# Print the resampled data\n",
    "print(\"Mean retweet count in each 2-hour interval:\")\n",
    "print(two_hour_tweets.head())\n",
    "print(\"\\nTotal like count in 3-day intervals:\")\n",
    "print(three_day_tweets.head())\n",
    "print(\"\\nBusiness day tweet counts:\")\n",
    "print(business_day_tweets.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling Window Analysis\n",
    "A rolling window allows us to calculate statistics over a sliding window of time. This is useful for smoothing out noise and identifying trends\n",
    "We create a rolling window using .rolling() on a column (or a Series) with **DateTimeIndex**. \n",
    "\n",
    "In the code below we first resample the data to days (`.resample('D')`) to show the number of rows (tweets) using `.size()` per day\n",
    "Then we create a rolling window of length 7 and length 30. See how the lines differ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 7-day rolling mean of tweet counts\n",
    "daily_tweets = df_time.resample('D').size() # this is a column (a Series) with DateTimeIndex which is resampled to days\n",
    "rolling_7d = daily_tweets.rolling(window=7, center=False).mean() # rolling mean of the daily tweet counts\n",
    "\n",
    "# Calculate 30-day rolling mean\n",
    "rolling_30d = daily_tweets.rolling(window=30, center=False).mean()\n",
    "\n",
    "# Plot the original daily counts and rolling means\n",
    "daily_tweets.plot(figsize=(12, 6), alpha=0.5, label='Daily Counts', legend=True)\n",
    "rolling_7d.plot(label='7-day Rolling Mean', linewidth=2, legend=True)\n",
    "rolling_30d.plot(label='30-day Rolling Mean', linewidth=2, legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The daily tweet counts (blue line) show high variability with many spikes.\n",
    "- The 7-day rolling mean (orange line) smooths out daily fluctuations while still capturing weekly patterns\n",
    "- The 30-day rolling mean (green line) shows the longer-term trend, smoothing out both daily and weekly variation. \n",
    "It reveals a more stable upward trend compared to the noisier daily data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Visualizations\n",
    "Visualization is a crucial tool in data analysis to understand the data, identify patterns, relationships and communicate findings. Common visualization techniques and the questions they answer:\n",
    "\n",
    "- **Line Plot**: How does a variable change over time?\t\n",
    "- **Histogram**: How values are distributed?\n",
    "- **Bar Plot**: How do groups compare?\n",
    "- **Box Plot**: How values are distributed AND groups compare (the Swiss army knife of plots)\n",
    "- **Scatter**: How are two metrics related?\n",
    "- **Pie Chart:** Don't use this crap\n",
    "\n",
    "I will not cover this topic extensively here because:  1) this notebook is already too long 2) the topic itself is too long 3) pandas & matplotlib is not the only way to create plots. I actually used to use 3rd party software to create plots and now use AI.\n",
    "\n",
    "Refer to the guide by if you wish to go deepr https://royal-statistical-society.github.io/datavisguide/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas, in fact, comes with built-in plotting functions that internally use Matplotlib, making it incredibly convenient to visualize data. Creating a plot can be as simple as calling df.plot() on a DataFrame.\n",
    "\n",
    "Depending on the type of plot, you may need to provide additional parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Line Plot\n",
    "Default plot when you call df.plot(). It will set the x-axis the index of the dataframe by default. The lines will show the columns.\n",
    "\n",
    "Our dataframe's index is id, which does not make sense for the line plot or any kind of plot as you see above, so we set the x-axis to be created_at. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[0:1000].plot(x='created_at')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line plot shows a line for each tweet. This is because we have not aggregated the data. (hence why I called it only on the first 1000 rows)\n",
    "\n",
    "We set the index to be `created_at`, resample by day, show the total number of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('created_at').resample('D').size().plot(title='Tweet Count Over Time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram\n",
    "Choose column to see its distribution of values. For instance, let's do retweet count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['retweet_count'].plot(kind='hist', figsize=(10, 6), title='Distribution of Retweet Counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks awful is not it? The data clearly does not follow a normal distribution. Most tweets receive no retweets, leading to a large spike at zero. Meanwhile, a small number of tweets receive very high retweet countsâ€”these are outliers. This results in a highly skewed distribution with a long tail to the right.\n",
    "\n",
    "There are several common ways to deal with this kind of skewed data:\n",
    "\n",
    "- Remove or cap outliers to focus on the bulk of the data.\n",
    "- Manually define retweet count categories, such as 0, 1â€“10, 11â€“1000, and 1000+, to summarize the data more meaningfully.\n",
    "- Redefine histogram bins to better capture the shape of the distribution.\n",
    "- Apply a logarithmic scale to the x-axis and/or y-axis to compress the range and make the distribution more interpretable.\n",
    "\n",
    "\n",
    "\n",
    "I usually prefer the last one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['retweet_count'].apply(np.log1p).plot(kind='hist', logy=True, yticks=[1,10,100,1000,10000,100000,1000000], xticks=[0,1,2,3,4,5,6,7,8,9,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used `apply(np.log1p)` to log-transform the retweet counts. The highest retweet count in the dataset is 16,080 (approximately `e^9.6`), while the lowest valuesâ€”0 and 1â€”are mapped to 0 after the transformation. So we could squeeze in the entire range of retweet counts to 0-10.   \n",
    "(Note: since `ln(0)` is undefined, `np.log1p(0)` conveniently returns 0.)\n",
    "\n",
    "We also applied a logarithmic scale to the **y-axis** (not to the actual values), so it increases exponentially (1, 10, 100, â€¦) instead of linearly. So we won't see a tower of 10^6 and a tiny bar representing 10^5.     \n",
    "Unfortunately, Pandas does not support setting a log scale on the **x-axis** directly, so we had to transform the x-values manually. (Try `logx=True` to see the abomination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bar Plot\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bar plot is good when you are comparing groups (i.e., categorical variables) over a single value (or multiple if you are using side-by-side bars.)\n",
    "\n",
    "Essentially, what we did with histograms were automatically creating groups by binning.\n",
    "But we could define categories of retweet counts manually and show their frequencies with a bar plot.\n",
    "Below, we first define `bins`, name them by `labels`.\n",
    "We create categories using pandas function `cut` which put the values (`df['retweet_count']` in this case) into bins you define.\n",
    "We then compute frequencies using `value_counts`\n",
    "And then call `plot(kind='bar')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define retweet count categories\n",
    "bins = [0, 1, 10, 100, 1000, float('inf')]\n",
    "labels = ['0', '1-10', '11-100', '101-1000', '1000+']\n",
    "\n",
    "# Create categories using pd.cut()\n",
    "retweet_categories = pd.cut(df['retweet_count'], bins=bins, labels=labels)\n",
    "\n",
    "# Count tweets in each category\n",
    "retweet_counts = retweet_categories.value_counts()\n",
    "\n",
    "# Create bar plot\n",
    "retweet_counts.plot(kind='bar', \n",
    "                   figsize=(10, 6),\n",
    "                   title='Distribution of Retweet Counts',\n",
    "                   xlabel='Retweet Count Range',\n",
    "                   ylabel='Number of Tweets')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You don't always need to define the categories manually. You can use any value as categories as long as it makes sense.    \n",
    "For instance let's see the number of tweets posted each day of the week the tweets.    \n",
    "Don't forget to aggregate!   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get day of week from created_at and count tweets\n",
    "tweet_counts_per_day = df['created_at'].dt.day_name().value_counts()\n",
    "\n",
    "# Plot using pandas plot()\n",
    "tweet_counts_per_day.plot(kind='bar', title='Number of Tweets by Day of Week')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most tweets are posted in mid-week and not on weekends where people have more free time. \n",
    "Not good for productivity!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pie Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple DataFrame with 'no' and 'no but' values\n",
    "data = {'Category': ['NO', 'NO but in yellow'], 'Count': [7, 3]}\n",
    "df_pie = pd.DataFrame(data)\n",
    "\n",
    "# Create pie chart using pandas plot()\n",
    "df_pie.plot(kind='pie', \n",
    "           y='Count',\n",
    "           labels=df_pie['Category'],\n",
    "           colors=['blue', 'yellow'],\n",
    "           autopct='%1.1f%%',\n",
    "           figsize=(8, 8),\n",
    "           title='Should You Use Pie Charts?')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Box Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box plots shows a distribution (and compare multiple distributions) without aggregation (finally, yay)    \n",
    "It does so by computing the minimum, the maximum, the median, and the first quartile (25% percentile) and the third quartile (75% percentile).         \n",
    "The box itself represents the interquartile range (IQR), which is the distance between the first and third quartiles.       \n",
    "The minimum and the maximum is computed by subtracting 1.5 times the IQR from the first quartile and adding 1.5 times the IQR to the third quartile respectively.  \n",
    "**They may not be the same as the minimum and the maximum value in the data**   \n",
    "Any data points that fall outside 1.5 times the IQR from the edges of the box are considered **outliers** and are typically shown as individual points. \n",
    "\n",
    "The image below summarizes it well:\n",
    "\n",
    "<img src=\"./data/box.jpg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The retweet count is mostly 0s, which means the minimum and the maximum will be 0s and everything else is an outlier. So a box plot does not make sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['retweet_count'].plot(kind='box')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, let's investigate our previous finding which indicates that most tweets were posted mid-week. \n",
    "Could this result be driven by outliers? For instance, did a bot go rogue and posted a million times on a Wednesday even though we have only 500k tweets, and inflated the numbers for Wednesday? \n",
    "Are **all* Wednesdays consistently busy on Twitter or Xverse? Or was this a one-off anomaly?\n",
    "Let's find out\n",
    "\n",
    " This time, we cannot rely on `df['day_of_week'].value_count()` because we do not want to aggregate by the day of the week (remember box plots do not operate on aggregated data.)    \n",
    " Still, we need to compute the number of rows/tweets for each day, and thus, need to aggregate by the date. Thus we group by the date and have a dataframe that has only the date and the number of tweets.      \n",
    " We still need to **retain the day of the week** information. One option is to regenerate it after grouping.       \n",
    " But a simple trick that decreases the amount of code is to add the categories to the `groupby` even though we do not intend to group by them.     \n",
    " If we run `groupby(['day_of_week', 'date'])` the data will be first grouped by the day of the week and then the date, and the resulting values will be the same as grouping by the date, but we retain the \"day_of_week\" column (in the index).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by day of the week, then date to get daily tweet counts\n",
    "df['date'] = df['created_at'].dt.date\n",
    "daily_counts = df.groupby(['day_of_week', 'date']).size().reset_index(name='tweet_count')\n",
    "daily_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a box plot comparing tweet counts across days of the week\n",
    "daily_counts.boxplot(column='tweet_count', \n",
    "                    by='day_of_week',\n",
    "                    figsize=(12, 6),\n",
    "                    ylabel='Number of Tweets per Day')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My suspicions were right - Wednesday does indeed have an outlier that inflates the total number of tweets on Wednesday.     \n",
    "It makes more sense to conclude that people tweet the most about ChatGPT on Tuesdays.       \n",
    "That said, the overall insight still holds: people tend to tweet more during the middle of the week.  \n",
    "Perhaps they surf on TikTok over the weekends instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scatter Plot\n",
    "Scatter plot is good when you want to see the relationship between two variables.    \n",
    "Like box plots, we do not aggregate and show individual data points instead, including outliers (yikes.)\n",
    "\n",
    "Let's see if there is a relationship between the number of likes and the number of retweets.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plot of likes vs retweets using pandas plot() with log scale on both axes\n",
    "df.plot(kind='scatter', \n",
    "        x='retweet_count', \n",
    "        y='like_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, I'm not sure if there is a relationship, because there are some points with many retweets but few likes, and vice versa. These create outliers along both axes.\n",
    "\n",
    "To address this, I will log-transform both axes.   \n",
    "I will also set `alpha = 0.1` to make the points more transparent, so overlapping data points form denser regions.   \n",
    "Finally, Iâ€™ll set `figsize=(6,6)` to produce a square plot, which makes it easier to interpret the relationship between the variables.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plot of likes vs retweets using pandas plot() with log scale on both axes\n",
    "df.plot(kind='scatter', \n",
    "        x='retweet_count', \n",
    "        y='like_count', \n",
    "        figsize=(6, 6),  # Changed to square dimensions\n",
    "        title='Likes vs Retweets (Log Scale)',\n",
    "        logx=True,\n",
    "        logy=True, \n",
    "        grid=True,\n",
    "        alpha=0.1,\n",
    "        xlim=(1, 10**4),\n",
    "        ylim=(1, 10**4), \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes there is a relationship between retweet counts and likes count (thank you captain obvious) but itâ€™s not perfectly linear â€” tweets receive more likes than retweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation\n",
    "Correlation measures the strength and direction of a linear relationship between two continuous variables.   \n",
    "It is essentially quantifying what we have just observed on the scatter plot.\n",
    "\n",
    "Pandas has a built-in function to compute correlation by `df.corr`. \n",
    "It computes **Pearson correlation coefficient** by default but can also compute Kendall Tau (`method='kendall'`) and Spearmank rank correlation (`method='spearman'`)\n",
    "All take values between -1 and 1. Higher absolute values indicate stronger correlation; the sign shows the direction (positive or negative).\n",
    "\n",
    "I leave going through their formulas and the logic to you as exercise.  \n",
    "But TL;DR: Pearson is based on values (e.g., like and retweet counts) and Spearman is based on rank (e.g., is the most liked tweet also the most retweeted)\n",
    "Spearman in a skewed dataset like ours. But it does not hurt if we compute and report all three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print correlation matrices with labels to identify which is which\n",
    "print(\"Pearson correlation (default):\")\n",
    "print(df[['retweet_count', 'like_count']].corr())\n",
    "print(\"\\nSpearman correlation:\")\n",
    "print(df[['retweet_count', 'like_count']].corr(method='spearman'))\n",
    "print(\"\\nKendall correlation:\")\n",
    "print(df[['retweet_count', 'like_count']].corr(method='kendall'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create different engagement thresholds\n",
    "thresholds = np.linspace(0, 1000, 100) # creates buckets of equal size between 0 and 1000\n",
    "\n",
    "# Create empty list to store correlations\n",
    "correlations = []\n",
    "\n",
    "# Calculate Spearman correlation for each threshold\n",
    "for threshold in thresholds: # for each threshold, filter the dataframe to only include rows where the engagement count is greater than the threshold\n",
    "    filtered_df = df[(df['engagement_count'] > threshold)]\n",
    "    corr = filtered_df[['retweet_count', 'like_count']].corr(method='spearman').iloc[0,1]\n",
    "    correlations.append(corr)\n",
    "\n",
    "# Create DataFrame for plotting\n",
    "corr_df = pd.DataFrame({\n",
    "    'threshold': thresholds,\n",
    "    'correlation': correlations\n",
    "})\n",
    "\n",
    "# Plot correlations\n",
    "corr_df.plot(\n",
    "    x='threshold',\n",
    "    y='correlation',\n",
    "    kind='line',\n",
    "    marker='o',\n",
    "    title='Spearman Correlation by Engagement Threshold',\n",
    "    xlabel='Minimum Engagement Threshold',\n",
    "    ylabel='Correlation Coefficient',\n",
    "    figsize=(10, 6),\n",
    "    grid=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the relationship decline for popular tweet that have more than 400 engagements. Weird "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the end of the analysis worksheet, hope enjoyed it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
